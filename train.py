# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lqf0-pyEnyabrHca_6MdQnazqkgd4jnW
"""

import sys
sys.path.append('/content/drive/MyDrive/Colab Notebooks/Naive_NMT')
import tensorflow as tf
from tensorflow.keras import backend as K
import time
from utils import accuracy_fn,loss_func
from encoder_decoder import Encoder,Decoder
from preprocessing import preprocessing

@tf.function
def train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer):
    with tf.GradientTape() as tape:
        # Get the encoder outputs
        en_outputs = encoder(input_seq, en_initial_states)
        # Set the encoder and decoder states
        en_states = en_outputs[1:]
        de_states = en_states
        # Get the encoder outputs
        de_outputs = decoder(target_seq_in, de_states)
        # Take the actual output
        logits = de_outputs[0]
        # Calculate the loss
        loss = loss_func(target_seq_out, logits)
        acc = accuracy_fn(target_seq_out, logits)

    variables = encoder.trainable_variables + decoder.trainable_variables
    # Calculate the gradients for the variables
    gradients = tape.gradient(loss, variables)
    # Apply the gradients and update the optimizer
    optimizer.apply_gradients(zip(gradients, variables))
    return loss, acc


def main_train(encoder, decoder, dataset, n_epochs, batch_size, optimizer,
               checkpoint, checkpoint_prefix): 
    losses = []
    accuracies = []
    for e in range(n_epochs):
        # Get the initial time
        start = time.time()
        # Get the initial state for the encoder
        en_initial_states = encoder.init_states(batch_size)
        # For every batch data
        for batch, (input_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):
            # Train and get the loss value 
            loss, accuracy = train_step(input_seq, target_seq_in, target_seq_out, en_initial_states, optimizer)                    
            if batch % 100 == 0:
                # Store the loss and accuracy values
                losses.append(loss)
                accuracies.append(accuracy)
                print('Epoch {} Batch {} Loss {:.4f} Acc:{:.4f}'.format(e + 1, batch, loss.numpy(), accuracy.numpy()))
                
        # saving (checkpoint) the model every 2 epochs
        if (e + 1) % 2 == 0:
            checkpoint.save(file_prefix = checkpoint_prefix)  
        print('Time taken for 1 epoch {:.4f} sec\n'.format(time.time() - start))
        
    return losses, accuracies

input_data_path='/content/drive/MyDrive/Colab Notebooks/Naive_NMT/dataset/input_data.txt'
target_data_path='/content/drive/MyDrive/Colab Notebooks/Naive_NMT/dataset/target_data.txt'
target_input_data_path='/content/drive/MyDrive/Colab Notebooks/Naive_NMT/dataset/target_input_data.txt'
with open(input_data_path) as f:
    input_data = f.read().splitlines()
with open(target_data_path) as f:
    target_data = f.read().splitlines()
with open(target_input_data_path) as f:
    target_input_data = f.read().splitlines()

NUM_SAMPLES = 80000
MAX_VOCAB_SIZE = 40000
EMBEDDING_DIM = 512
HIDDEN_DIM=1024
BATCH_SIZE = 32
EPOCHS = 20 

proc=preprocessing()
dataset,word2idx_inputs,word2idx_outputs=proc.text2tf_dataset(input_data,target_input_data,target_data
                                                              ,BATCH_SIZE=BATCH_SIZE)
num_words_inputs = len(word2idx_inputs) + 1
num_words_output = len(word2idx_outputs) + 1

#Create the encoder
encoder = Encoder(num_words_inputs, EMBEDDING_DIM, HIDDEN_DIM)
decoder = Decoder(num_words_output, EMBEDDING_DIM, HIDDEN_DIM)

optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)
checkpoint_dir = '/content/drive/MyDrive/Colab Notebooks/Naive_NMT/ck'
checkpoint = tf.train.Checkpoint(optimizer=optimizer,
                                 encoder=encoder,
                                 decoder=decoder)

losses, accuracies = main_train(encoder, decoder, dataset, EPOCHS, BATCH_SIZE, optimizer, checkpoint, checkpoint_dir)